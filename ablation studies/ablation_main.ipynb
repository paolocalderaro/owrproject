{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ablation_main.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"akp--Dj9RBTj"},"source":["#import libraries and package\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"wkBUBd8bQ6HE"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import numpy as np\n","import sys\n","import os\n","np.set_printoptions(threshold=sys.maxsize) #needed to print correctly the logs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_OSaUaoksjR"},"source":["righe di codice utili per importare da altri jupyter notebook delle funzioni. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i2MXsnVKxbzx","executionInfo":{"status":"ok","timestamp":1625042826067,"user_tz":-120,"elapsed":14935,"user":{"displayName":"Paolo Calderaro","photoUrl":"","userId":"07517975176708291925"}},"outputId":"1d207c53-278a-4d52-abe2-bad592b4b395"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrWrAj7bjtv5","executionInfo":{"status":"ok","timestamp":1625042832797,"user_tz":-120,"elapsed":5629,"user":{"displayName":"Paolo Calderaro","photoUrl":"","userId":"07517975176708291925"}},"outputId":"b824c8ae-1fec-4ff9-acc6-aff3a6fa5d2b"},"source":["%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto\"\n","!pip install import-ipynb\n","\n","import import_ipynb "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MLDL Group project/File colab/File progetto\n","Collecting import-ipynb\n","  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=df0a0d5d312e05a88bc59b6cc11428920a921b6bd2196a051cc93d5dc3657e4f\n","  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dKzmzOrAkGiM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625043633446,"user_tz":-120,"elapsed":226,"user":{"displayName":"Paolo Calderaro","photoUrl":"","userId":"07517975176708291925"}},"outputId":"13344bee-b010-46eb-eb9b-1534bbc5a982"},"source":["#retrieve from folder 'data'\n","%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/data\"                          \n","from DataClean import Cifar100 as datasetManager   # from **nome jupyter notebook** import **classe o metodo da importare**\n","from Manager import Manager\n","\n","#retrieve from folder 'models'\n","%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/models\"\n","from resnet import resnet32\n","from resnetAnnoCorrente import resnet32 as resnet32new\n","from LwFMC import LWFoptimized\n","from iCaRL_vanilla import iCaRL\n","from iCaRL_vanilla import Exemplars\n","from iCaRL_vanilla import iCaRL_herd\n","\n","\n","#retrieve from folder 'logs'\n","%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/logs\"\n","from Save_logs import save_logs\n","\n","#retrieve from folder 'ablation'\n","%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/ablation studies/ablation_models\"\n","from iCaRL_KNN import iCaRL_knn\n","from iCaRL_SVM import iCaRL_svm\n","\n","\n","# return in the main project folder\n","%cd \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/MLDL Group project/File colab/File progetto/data\n","/content/drive/MyDrive/MLDL Group project/File colab/File progetto/models\n","/content/drive/MyDrive/MLDL Group project/File colab/File progetto/logs\n","/content/drive/MyDrive/MLDL Group project/File colab/File progetto/ablation studies/ablation_models\n","/content/drive/MyDrive/MLDL Group project/File colab/File progetto\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lwY_iVOV6NpA"},"source":["utile quando trasforemero tutto in .py, ad ora non serve eseguire:"]},{"cell_type":"code","metadata":{"id":"DgM3-hQ86IXY"},"source":["# #righe utili ad importare file dalle cartelle logs, models, utils\n","# file_location = \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/logs/\"\n","# sys.path.append(os.path.abspath(file_location))\n","\n","# file_location = \"/content/drive/MyDrive/MLDL Group project/File colab/File progetto/models/\"\n","# sys.path.append(os.path.abspath(file_location))\n","\n","# #elimina gli as se tutto funziona come dovrebbe\n","# from Save_logs import save_logs\n","# from resnet import resnet32 as resnetPy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVIwT_NfRboW"},"source":["# Arguments"]},{"cell_type":"code","metadata":{"id":"vl63oUufRbJN"},"source":["DEVICE=torch.device(\"cuda:0\")\n","DIR='/content/data'\n","\n","RANDOM_SEED = [1993,30,423]   \n","NUM_CLASSES = 100      \n","\n","# Training parameter: \n","BATCH_SIZE = 64         # Batch size \n","NUM_EPOCHS = 70         # Total number of training epochs\n","LR = 2                  # Initial learning rate\n","MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n","WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n","MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n","                        # Decrease the learning rate by gamma at each milestone\n","GAMMA = 0.2             # Gamma factor from iCaRL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD6XReTJSCX2"},"source":["train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),  \n","                                      transforms.RandomHorizontalFlip(),     \n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","\n","test_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SULeiW0AiKXw"},"source":["# Loss Studies"]},{"cell_type":"markdown","metadata":{"id":"GbXHRQVTHOXs"},"source":["## LWF loss\n","\n"]},{"cell_type":"code","metadata":{"id":"xZt_R0JzHO08"},"source":["RUN_NAME = 'iCarl_ablation_distillation_LWF'\n","\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","#### customize hyperparameter for tuning ######\n","############ (debug purpose) ##################\n","LR = 0.1\n","NUM_EPOCHS = 70\n","WEIGHT_DECAY = 1e-4\n","###############################################\n","\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","   #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_LWF(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","\n","        ############## training + log generation ###################        \n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqIMew-8oAuA"},"source":["## L1 and L2 loss"]},{"cell_type":"code","metadata":{"id":"ttsMXogXn_fj"},"source":["CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","NUM_EPOCHS = 70\n","\n","#customize hyperparameter for tuning:\n","WEIGHT_DECAY = 1e-4\n","LR = 0.3\n","\n","#set loss to test: CHOOSE HERE L1/L2 LOSS\n","cls_loss = nn.CrossEntropyLoss()\n","dist_loss = nn.MSELoss()              #nn.L1loss() here to test L1 loss\n","\n","# Initialize log file:\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","  #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_loss(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, \n","                     train_transform, test_transform, cls_loss, dist_loss)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","        ############## training + log generation ###################\n","\n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","      \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mS27GMzIiPh6"},"source":["# classifier Studies\n","class_params are loaded with the best perfomance set found. "]},{"cell_type":"markdown","metadata":{"id":"rveSKWUZHft9"},"source":["## Ablation studies: SVM as classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"j7C-FMz2HioR"},"source":["RUN_NAME = 'iCarl_SVM_alldata'\n","\n","#### debug purpose ######\n","NUM_RUNS = len(RANDOM_SEED)\n","NUM_EPOCHS = 70\n","#########################\n","\n","###chose hyperparameters for ablation: ###\n","all_data = True\n","criterion = nn.BCEWithLogitsLoss()\n","class_params = {'kernel':'linear', 'tol':0.0001}\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","  #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n"," \n","  net = resnet32()\n","  icarl = iCaRL_svm(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES,\n","                    GAMMA, NUM_EPOCHS, BATCH_SIZE, \n","                    train_transform, test_transform,  criterion, class_params, all_data)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","    \n","        ############## training + log generation ###################\n","        train_logs = icarl.incremental_train(split_i, train_data_split, val_data_split)\n","\n","        torch.save(icarl, \"model_svm.pt\")          \n","       \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4OvJBQ7H14z"},"source":["## Ablation studies: KNN as classifier"]},{"cell_type":"code","metadata":{"id":"i-Q0Bk4iH2fn"},"source":["RUN_NAME = 'iCarl_knn_all_data'\n","\n","#### debug purpose ######\n","NUM_RUNS = len(RANDOM_SEED)\n","NUM_EPOCHS = 70\n","#########################\n","\n","#set if we want to train classifier with all data or only exemplars\n","all_data = True\n","criterion = nn.BCEWithLogitsLoss()\n","class_params = {\n","                'n_neighbors': 10,\n","                'weights': 'distance'\n","                      }\n","\n","logs = [[] for _ in range((len(RANDOM_SEED)))]\n","\n","for run_i in range(len(RANDOM_SEED)):\n","    \n","    #download cifar100:\n","    train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","    test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","    \n","    net = resnet32()\n","    icarl_knn = iCaRL_KNN(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES,\n","                          GAMMA, NUM_EPOCHS, BATCH_SIZE,\n","                          train_transform, test_transform,  criterion, class_params, all_data)\n","\n","    for split_i in range(10):\n","        print(f\"## Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) ##\")\n","        \n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      \n","                                              random_state=RANDOM_SEED[run_i],\n","                                              test_size = 0.1)  #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)       #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)           #the original train_dataset\n","\n","\n","        # training + log generation\n","        icarl_knn.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        logs[run_i].append({})\n","\n","        # test + log generation\n","        acc, all_targets, all_preds = icarl_knn.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]}]}