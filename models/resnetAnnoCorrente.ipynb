{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnetAnnoCorrente.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbQIsf4JEHP6Kp63hWYi/Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1jS0U6l3si8t"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","from torch.autograd import Variable\n","\n","__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n","\n","def _weights_init(m):\n","    classname = m.__class__.__name__\n","    #print(classname)\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight)\n","\n","class LambdaLayer(nn.Module):\n","    def __init__(self, lambd):\n","        super(LambdaLayer, self).__init__()\n","        self.lambd = lambd\n","\n","    def forward(self, x):\n","        return self.lambd(x)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, option='B'):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes:\n","            if option == 'A':\n","                \"\"\"\n","                For CIFAR10 ResNet paper uses option A.\n","                \"\"\"\n","                self.shortcut = LambdaLayer(lambda x:\n","                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n","            elif option == 'B':\n","                self.shortcut = nn.Sequential(\n","                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                     nn.BatchNorm2d(self.expansion * planes)\n","                )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n","        self.fc = nn.Linear(64, num_classes)\n","\n","        self.apply(_weights_init)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, out.size()[3])\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","        return out\n","\n","\n","def resnet20():\n","    return ResNet(BasicBlock, [3, 3, 3])\n","\n","\n","def resnet32():\n","    return ResNet(BasicBlock, [5, 5, 5])\n","\n","\n","def resnet44():\n","    return ResNet(BasicBlock, [7, 7, 7])\n","\n","\n","def resnet56():\n","    return ResNet(BasicBlock, [9, 9, 9])\n","\n","\n","def resnet110():\n","    return ResNet(BasicBlock, [18, 18, 18])\n","\n","\n","def resnet1202():\n","    return ResNet(BasicBlock, [200, 200, 200])\n","\n","\n","def test(net):\n","    import numpy as np\n","    total_params = 0\n","\n","    for x in filter(lambda p: p.requires_grad, net.parameters()):\n","        total_params += np.prod(x.data.numpy().shape)\n","    print(\"Total number of params\", total_params)\n","    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n","\n","\n","if __name__ == \"__main__\":\n","    for net_name in __all__:\n","        if net_name.startswith('resnet'):\n","            print(net_name)\n","            test(globals()[net_name]())\n","            print()"],"execution_count":null,"outputs":[]}]}