{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCaRL_LWF.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9HTSG2q8HQVr1uQea+Jp2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wyzKp1zuJJjb"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","sigmoid = nn.Sigmoid() # Sigmoid function\n","softmax = nn.Softmax(dim=None)\n","logsoftmax = nn.LogSoftmax()\n","\n","\n","def MultiClassCrossEntropy(logits, targets, T):\n","\t# Ld = -1/N * sum(N) sum(C) softmax(label) * log(softmax(logit))\n","\ttargets = Variable(targets.data, requires_grad=False).cuda()\n","\toutputs = torch.log_softmax(logits/T, dim=1)   # compute the log of softmax values\n","\ttargets = torch.softmax(targets/T, dim=1)\n","\n","\toutputs = torch.sum(outputs * targets, dim=1, keepdim=False)\n","\toutputs = -torch.mean(outputs, dim=0, keepdim=False)\n"," \n","\treturn Variable(outputs.data, requires_grad=True).cuda()\n"," \n"," \n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","        # exemplars = [\n","        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n","        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n","        #     ...\n","        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n","        # ]\n","\n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)  #lista.extend() aggiunge vari elementi alla lista (corrispettivo di append() che ne aggiunge uno solo)\n","            self.targets.extend([y] * len(exemplar_y))  # return: [y,y,y,y, ... ] until len(exemplar_y)\n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","class iCaRL_LWF:\n","\n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        self.exemplars = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","\n","    def classify(self, batch, train_dataset=None):\n","        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n","        classes observed so far.\n","        Args:\n","            batch (torch.tensor): batch to classify\n","        Returns:\n","            label (int): class label assigned to the image\n","        \"\"\"\n","        \n","        #Ottenimento rappresentazioni per il batch da classificare\n","        batch_features = self.extract_features(batch) # (batch size, 64) \n","        for i in range(batch_features.size(0)):       # per ogni singola sample\n","            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n","        batch_features = batch_features.to(self.device)\n","        \n","        #Ottenimento prototipes\n","        if self.cached_means is None: #si fa per il primo batch del test set, poi si conservano in memoria (per questo split)\n","            print(\"Computing mean of exemplars... \", end=\"\")\n","\n","            self.cached_means = []\n","\n","            # Number of known classes\n","            num_classes = len(self.exemplars)\n","\n","            # Compute the means of classes with all the data available,\n","            # including training data which contains samples belonging to\n","            # the latest 10 classes. This will remove noise from the mean\n","            # estimate, improving the results.\n","            if train_dataset is not None:\n","                train_features_list = [[] for _ in range(10)]\n","\n","\n","                #inserimento rappresentazioni degli elementi nuovi in classi in 10 liste\n","                #utilizzando tutto il train_dataset\n","                for train_sample, label in train_dataset: \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform) #notare che la trasformazione è quella del set, in quanto bisogna solo normalizzare\n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","\n","            # Compute means of exemplars for all known classes\n","\n","            for y in range(num_classes):\n","                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)): #se label di una nuova classe\n","                    features_list = train_features_list[y % 10]\n","                else:\n","                    features_list = []\n","                \n","                #credo si debba entrare in questo \"for\" solo per elementi delle vecchie classi (exemplars), \n","                #sarebbe più corretto allinearlo con l'else\n","                # in questo for stiamo recuperando tutti gli exemplars di una classe\n","                for exemplar in self.exemplars[y]: \n","                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n","                    features = features/features.norm() # Normalize the feature representation of the exemplar\n","                    features_list.append(features)      \n","                \n","                #qui calcoliamo effettivamente la media per ogni classe\n","                features_list = torch.stack(features_list)\n","                class_means = features_list.mean(dim=0) #media su ogni feature \n","                class_means = class_means/class_means.norm() # Normalize the class means\n","\n","                #inseriamo la media calcolata per ogni classe precedente nella variabile\n","                #cached_means\n","                self.cached_means.append(class_means)\n","            \n","            self.cached_means = torch.stack(self.cached_means).to(self.device)\n","            print(\"done\")\n","        \n","        #Classificazione\n","        preds = []\n","        for i in range(batch_features.size(0)): #per ogni immagine nel batch\n","            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1) #calcoliamo la differenza con ogni media/ogni classe\n","            preds.append(torch.argmin(f_arg)) #indice (quindi label) della media più vicina alla rappresentazione dell'immagine\n","                                              #che corrisponde alla nostra predizione\n","        \n","        return torch.stack(preds)\n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n","        \"\"\"Extract features from single sample or from batch.\n","        \n","        Args:\n","            sample (PIL image or torch.tensor): sample(s) from which to\n","                extract features\n","            batch (bool): if True, sample is a torch.tensor containing a batch\n","                of images with dimensions (batch_size, 3, 32, 32)\n","            transform: transformations to apply to the PIL image before\n","                processing\n","        Returns:\n","            features: torch.tensor, 1-D of dimension 64 for single samples or\n","                2-D of dimension (batch_size, 64) for batch\n","        \"\"\"\n","\n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\" #se la condizione non è soddisfatta viene stampato questo messaggio\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False: # Treat sample as single PIL image (di fatto trasformiamo la nostra immagine in un batch di 1 immagine)\n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","       \n","        ##############################\n","        #\n","        # usiamo come feature mapping il feature mapping utilizzato\n","        # dalla resnet: il batch di immagini passa attraverso tutti i layer\n","        # della resnet, eccetto il fc layer finale\n","        #(che é adibito alla classificazione)\n","        #\n","        #############################\n","\n","        if self.VALIDATE: # nella validation si usa il miglior modello\n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)   \n","\n","        if batch is False:\n","            features = features[0] # se singola immagine return 1D dimensional tensor (prendo la prima e unica riga)\n","\n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","        \"\"\"Adjust internal knowledge based on the additional information\n","        available in the new observations.\n","        Args:\n","            split (int): current split number, counting from zero\n","            train_dataset: dataset for training the model\n","            val_dataset: dataset for validating the model\n","        Returns:\n","            train_logs: tuple of four metrics (train_loss, train_accuracy,\n","            val_loss, val_accuracy) obtained during network training\n","        \"\"\"\n","\n","        if split is not 0:\n","            # Increment the number of output nodes for the new network by 10\n","            #starting from 1 (at run 0 we already have 10 output nodes)\n","            self.increment_classes(10)\n","\n","        # Improve network parameters upon receiving new classes. Effectively\n","        # train a new network starting from the current network parameters.\n","\n","        ##### inserire qui il contenuto di update_representation ############\n","        train_logs = self.update_representation(train_dataset, val_dataset) #<-- 1 costruzione dataset = concat(exemplars, train_dataset); 2 train; 3 copia rete\n","        #####################################################################\n","\n","        # Compute the number of exemplars per class\n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","        # Construct exemplar set for new classes\n","        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m) #dovremmo usare l'herding\n","        self.exemplars.extend(new_exemplars)\n","\n","        return train_logs\n","\n","    #questa funzione é l'implementazione dell'algoritmo 3 di iCarl\n","    def update_representation(self, train_dataset, val_dataset): #dropperei questo metodo inserendo i passaggi esplictamente nel precedente\n","        \"\"\"Update the parameters of the network.\n","        Args:\n","            train_dataset: dataset for training the model\n","            val_dataset: dataset for validating the model\n","        Returns:\n","            train_logs: tuple of four metrics (train_loss, train_accuracy,\n","            val_loss, val_accuracy) obtained during network training\n","        \"\"\"\n","\n","        # Combine the new training data with existing exemplars:\n","        ### \n","        # ogni volta che trainiamo su 10 classi nuove, trainiamo il modello con i dati nuovi \n","        # riferenti solo alle nuove 10 classi, ma anche con tutti gli exemplar; per questo\n","        # ora andiamo a concatenare train_dataset e exemplars_dataset e trainiamo sulla concatenazione\n","        # (vedi algorithm 3 di iCarl)\n","        ###\n","\n","        # len(self.exemplars) = num classi viste\n","        # len(self.exemplars[y]) = num elementi di una classe\n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n","\n","        # Keep a copy of the current network in order to compute its outputs for\n","        # the distillation loss while the new network is being trained.\n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","    def construct_exemplar_set_rand(self, dataset, m):\n","        \"\"\"Randomly sample m elements from a dataset without replacement.\n","        Args:\n","            dataset: dataset containing a split (samples from 10 classes) from\n","                which to take exemplars\n","            m (int): target number of exemplars per class\n","        Returns:\n","            exemplars: list of samples extracted from the dataset\n","        \"\"\"\n","\n","        dataset.dataset.disable_transform()\n","        \n","        #storing images of a split\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        exemplars = [[] for _ in range(10)]\n","        \n","        #sampling\n","        for y in range(10):\n","            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Randomly choose m samples from samples[y] without replacement\n","            exemplars[y] = random.sample(samples[y], m)\n","\n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","\n","        return exemplars\n","\n","    def construct_exemplar_set_herding(self, dataset, m): #impiegata nel paper\n","        \"\"\"Extract m elements from a dataset by herding.\n","        Args:\n","            dataset: dataset containing a split (samples from 10 classes) from\n","                which to take exemplars\n","            m (int): target number of exemplars per class\n","        Returns:\n","            exemplars: list of samples extracted from the dataset\n","        \"\"\"\n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  #?? shape(0)\n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n","                if k == 1: # No exemplars chosen yet, sum to 0 vector\n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k * samples_features + f_sum, dim=1) #N.B. samples_features è un tensore, size = (num di sample, 64), gli altri vettori di 64 elem\n","                \n","                #Credo debba essere: \n","                # f_arg = torch.norm(features_mean - 1/k * (samples_features + f_sum), dim=1)\n","\n","                # Mask exemplars that were already taken, as we do not want to store the\n","                # same exemplar more than once\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) #rende invalidi gli elementi dove mask = 1, escludendo dunque gli indici degli elementi già scelti come exmplars, esempio sotto\n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","        \"\"\"Procedure for removing exemplars from a given set.\n","        Args:\n","            exemplar_set (set): set of exemplars belonging to a certain class\n","            m (int): target number of exemplars\n","        Returns:\n","            exemplar_set: reduced exemplar set\n","        \"\"\"\n","\n","        return exemplar_set[:m]\n","    \n","    #\n","    #train is the same of standard train routine\n","    #\n","    def train(self, train_dataset, val_dataset):\n","        \"\"\"Train the network for a specified number of epochs, and save\n","        the best performing model on the validation set.\n","        \n","        Args:\n","            train_dataset: dataset for training the model\n","            val_dataset: dataset for validating the model\n","        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n","            val_loss, val_accuracy) obtained during network training. If\n","            validation is enabled, return scores of the best epoch, otherwise\n","            return scores of the last epoch.\n","        \"\"\"\n","\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        # Create DataLoaders for training and validation, in questo caso sono istanziati da iCaRL mentre le altre volte avveniva esternamente\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  # Calling this optimizes runtime\n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","        \"\"\"Trains model for one epoch.\n","        \n","        Args:\n","            current_epoch (int): current epoch number (begins from 1)\n","        Returns:\n","            train_loss: average training loss over all batches of the\n","                current epoch.\n","            train_accuracy: training accuracy of the current epoch over\n","                all samples.\n","        \"\"\"\n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","        \"\"\"Train network for a batch. Loss is applied here.\n","        Args:\n","            batch: batch of data used for training the network\n","            labels: targets of the batch\n","        Returns:\n","            loss: output of the criterion applied\n","            running_corrects: number of correctly classified elements\n","        \"\"\"\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","        \n","\n","        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n","\n","        outputs = self.net(batch)\n","\n","        cls_loss = nn.CrossEntropyLoss()(outputs, labels)\n","             \n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            loss = cls_loss\n","\n","        else:\n","            dist_target = self.old_net(batch)           # target per la distillation\n","            logits_dist = outputs[:, :num_classes-10]   # neuroni corrispondenti alle vecchie classi, sui quali applicare la distillation\n","            #distillation da parametrizzare\n","            dist_loss = MultiClassCrossEntropy(logits_dist, dist_target, 2)\n","            loss = dist_loss + cls_loss                 # distillation e classification loss\n","        \n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over NEW IMAGES, not over all images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self): #la validation non usa gli exemplars, ma gli output della rete: sarà corretto? \n","                        # risposta di Fontanel: si, nella letteratura non ci sono 'standard', dunque si puo validare\n","                        # sia in maniera 'standard' (usando un validation_loss) che con gli exemplars; qui viene usaot\n","                        # un loss\n","        \"\"\"Validate the model.\n","        \n","        Returns:\n","            val_loss: average loss function computed on the network outputs\n","                of the validation set (val_dataloader).\n","            val_accuracy: accuracy computed on the validation set.\n","        \"\"\"\n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)\n","            # outputs = torch.log_softmax(outputs/self.T, dim = 1)\n","\n","            cls_loss = nn.CrossEntropyLoss()(outputs, labels)      # ho messo solo la classification loss, considerando che qui non abbiamo\n","                                                                   # output della vecchia rete, vogliamo solo classificare (sarà corretto?)\n","\n","\n","            running_val_loss += cls_loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","        \"\"\"Test the model.\n","        Args:\n","            test_dataset: dataset on which to test the network\n","            train_dataset: training set used to train the last split, if\n","                available\n","        Returns:\n","            accuracy (float): accuracy of the model on the test set\n","        \"\"\"\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # Clear mean of exemplars cache\n","        self.cached_means = None #ogni volta che si testa la rete vanno ricomputati i prototipes: i parametri della rete sono cambiati dopo il training --> cambieranno le rappresentazioni delle immagini\n","        \n","        # Disable transformations for train_dataset, if available, as we will\n","        # need original PIL images from which to extract features.\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","        \n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","    def test_without_classifier(self, test_dataset):\n","        \"\"\"Test the model without classifier, using the outputs of the\n","        network instead.\n","        Args:\n","            test_dataset: dataset on which to test the network\n","        Returns:\n","            accuracy (float): accuracy of the model on the test set\n","        \"\"\"\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        all_preds = torch.tensor([]) # to store all predictions\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","        \n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # Forward Pass\n","            with torch.no_grad():\n","                if self.VALIDATE:\n","                    outputs = self.best_net(images)\n","                else:\n","                    outputs = self.net(images)\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update Corrects\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            # Append batch predictions\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (hybrid1): {accuracy}\")\n","\n","        return accuracy, all_targets, all_preds\n","    \n","    def increment_classes(self, n=10):\n","        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n","\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n","        \"\"\"Return the number of output neurons of the current network.\"\"\"\n","\n","        return self.net.fc.out_features\n","    \n","    def feature_neurons_count(self):\n","        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n","\n","        return self.net.fc.in_features\n","    \n","    def to_onehot(self, targets):\n","        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n","        Args:\n","            targets: dataloader.dataset.targets of the new task images\n","        \"\"\"\n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n","    def network_params(self):\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        return weight, bias\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","from sklearn.svm import SVC\n","\n","\n","sigmoid = nn.Sigmoid() # Sigmoid function\n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","        # exemplars = [\n","        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n","        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n","        #     ...\n","        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n","        # ]\n","\n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)  #lista.extend() aggiunge vari elementi alla lista (corrispettivo di append() che ne aggiunge uno solo)\n","            self.targets.extend([y] * len(exemplar_y))  # return: [y,y,y,y, ... ] until len(exemplar_y)\n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n"],"execution_count":null,"outputs":[]}]}